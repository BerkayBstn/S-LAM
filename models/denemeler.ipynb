{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38221c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import attn \n",
    "import torch\n",
    "import numpy as np\n",
    "torch.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce5c8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAM = attn.LocalAttention(\n",
    "    neigh_size = 50,\n",
    "    splits = 4,\n",
    "    device = 'cpu',\n",
    "    output_attention=True,\n",
    ").eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84028f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = attn.AttentionLayer(\n",
    "    attention = LAM,\n",
    "    window_size=200,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    d_k = 64,\n",
    "    d_v = 64,\n",
    "    mix = False\n",
    ").eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d1a472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 64])\n"
     ]
    }
   ],
   "source": [
    "Q = torch.randn(5, 400, 64).float()   # (batch_size, seq_length, d_model)\n",
    "K = torch.randn(5, 400, 64).float()   # (batch_size, seq_length, d_model)\n",
    "V = torch.randn(5, 400, 64).float()   # (batch_size, seq_length, d_model)\n",
    "\n",
    "output, A = attention(Q, K, V)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "452f2705",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Q_split, K_split, V_split, S, S_masked, A, V_split, output_refined \u001b[38;5;241m=\u001b[39m \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\beb171\\Anaconda3\\envs\\transSM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\beb171\\Anaconda3\\envs\\transSM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\TransformerSurrogate\\S-LAM\\models\\attn.py:738\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[1;34m(self, queries, keys, values, **kwargs)\u001b[0m\n\u001b[0;32m    735\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_projection(values)\u001b[38;5;241m.\u001b[39mview(B, S, H, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m# Applies the attention mechanism.\u001b[39;00m\n\u001b[1;32m--> 738\u001b[0m out, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_attention(\n\u001b[0;32m    739\u001b[0m     queries,\n\u001b[0;32m    740\u001b[0m     keys,\n\u001b[0;32m    741\u001b[0m     values,\n\u001b[0;32m    742\u001b[0m     debugging\u001b[38;5;241m=\u001b[39mdebugging\n\u001b[0;32m    743\u001b[0m )\n\u001b[0;32m    745\u001b[0m \u001b[38;5;66;03m# Places for each variable, all the outputs of the heads for that variable continuously.\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmix:\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "Q_split, K_split, V_split, S, S_masked, A, V_split, output_refined = attention(Q, K, V, debugging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3ea08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006c018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b40fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567189d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c76f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randn(5, 200, 64).float()   # (batch_size, seq_length, d_model)\n",
    "K = torch.randn(5, 200, 64).float()   # (batch_size, seq_length, d_model)\n",
    "V = torch.randn(5, 200, 64).float()   # (batch_size, seq_length, d_model)\n",
    "\n",
    "output = attention(Q, K, V)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randn(5, 600, 64).float()   # (batch_size, seq_length, d_model)\n",
    "K = torch.randn(5, 600, 64).float()   # (batch_size, seq_length, d_model)\n",
    "V = torch.randn(5, 600, 64).float()   # (batch_size, seq_length, d_model)\n",
    "\n",
    "output = attention(Q, K, V)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9526ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "torch.full((5, 5), -sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b702e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "neigh_size = 3\n",
    "full_mask = torch.tril(torch.ones((window_size,window_size), dtype=torch.double)) - torch.tril(torch.ones((window_size,window_size), dtype=torch.double),diagonal=-neigh_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d481142",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699b491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tril(torch.ones((window_size,window_size), dtype=torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9793774",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tril(torch.ones((window_size,window_size), dtype=torch.double),diagonal=-neigh_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a1d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = torch.rand((4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d26444",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.einsum('ii, jj -> ij', matrix, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.einsum('ii', matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "B S L H E @ B S T H E -> B S L H T\n",
    "            B S E H T\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a638306",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ab9e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_indexes = []\n",
    "B = 8\n",
    "L = 100\n",
    "H = 4\n",
    "E = 3\n",
    "neigh_size = 5\n",
    "l1 = neigh_size\n",
    "splits = L // neigh_size\n",
    "\n",
    "for j in torch.arange(0, splits):\n",
    "    for i in torch.arange(0, l1):\n",
    "        split_indexes.append(i+l1*j)\n",
    "split_indexes_Q = torch.tensor(split_indexes).reshape(splits, l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd4165fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix SI such that for split i, the block i of K has the rows with index in SI[i,:]\n",
    "split_indexes = [] \n",
    "split_indexes_KV_end = None\n",
    "for j in torch.arange(0, splits):\n",
    "    for i in torch.arange(0, l1+neigh_size):\n",
    "        split_indexes.append(i+l1*j-neigh_size)\n",
    "split_indexes_KV = torch.tensor(split_indexes).reshape(splits, l1+neigh_size)\n",
    "split_indexes_KV[0] = torch.cat((torch.zeros(neigh_size), torch.arange(0, l1)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ea56f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.arange(0, B*L*H*E).reshape(B, L, H, E)\n",
    "Q_expand = queries.unsqueeze(0).expand(splits, B, L, H, E).permute(1,0,2,3,4)\n",
    "Q_split = Q_expand[:, torch.arange(splits).unsqueeze(1), split_indexes_Q, :].transpose(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b563cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = queries.clone()\n",
    "\n",
    "# Split keys (here we need more rows per block to capture the attention scores of queries and those elements of keys at positions j l1 - neigh_size to j l1.\n",
    "K_expand = keys.unsqueeze(0).expand(splits, B, L, H, E).permute(1,0,2,3,4)\n",
    "K_split = K_expand[:, torch.arange(splits).unsqueeze(1), split_indexes_KV, :].transpose(0,1)\n",
    "K_split[0,:,:neigh_size,:,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f987cb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 8, 5, 4, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34da05ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 8, 10, 4, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f923e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 8, 4, 5, 10])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbe4346a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[2, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a6d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f64816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d510154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_split shape: torch.Size([20, 8, 5, 4, 3])\n",
      "K_split shape: torch.Size([20, 8, 10, 4, 3])\n",
      "S shape: torch.Size([20, 8, 4, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_split shape:\", Q_split.shape)\n",
    "print(\"K_split shape:\", K_split.shape)\n",
    "S = torch.einsum(\"sblhe,sbthe->sbhlt\", Q_split, K_split)\n",
    "print(\"S shape:\", S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b88657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.softmax(S.float(), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(A, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44625bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transSM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
